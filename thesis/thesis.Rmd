---
title: Nebula 
subtitle: Comparing two waves of cloud computing 
author: Marius Nilsen Kluften  
documentclass: report
classoption:
  - table
date: 2024
output: 
  pdf_document:
    number_sections: true
    latex_engine: lualatex
    citation_package: biblatex
    fig_caption: true

header-includes: 
  - \AtBeginDocument{\let\maketitle\relax}
  - \usepackage{lastpage} % Calculate the last page of the document
  - \usepackage{tocbibind} % Insert custom table of content items
  - \usepackage{graphicx} % import figure
  - \usepackage{float} % Allow for more granular control of float of figures
  - \usepackage{setspace}
  - \usepackage{lipsum} % Generate lorem ipsum text
  - \usepackage{acronym}
  - \usepackage[fontsize=12pt]{scrextend}
  - \usepackage{fontspec} % change to system font
  - \usepackage{tcolorbox} % Fancy text boxes that are pretty
  - \tcbset{
      definitionstyle/.style={
        colback=orange!5!white,
        colframe=orange!80!black,
        top=0.25cm,
        bottom=0.25cm,
        before=\vspace{0.5cm},
        after=\vspace{0.5cm}
      }
    }
  - \usepackage{todonotes}

  - \usepackage[style=apa, natbib=true]{biblatex}
  - \addbibresource{nebula.bib}
  - \addbibresource{manual-nebula.bib}
  - \DefineBibliographyExtras{english}{}
  - \usepackage{csquotes}


  - \usepackage{hyperref}
  - \usepackage{cleveref}
  - \usepackage{booktabs}
  - \usepackage{array} 

  - \onehalfspacing

  - \usepackage[a4paper,width=150mm,top=25mm,bottom=25mm]{geometry}
  - \usepackage{fancyhdr}
  - \pagestyle{fancy}
  - \fancyhf{}
  - \fancyfoot[C]{}
  - \usepackage[bottom]{footmisc}
  - \tcbuselibrary{theorems}


  # - \usepackage{tocloft}
  # - \renewcommand\cftchapaftersnum{.}
  # - \renewcommand\cftsecaftersnum{.}
  #
  # - \renewcommand\thesection{\arabic{section}}
  # - \renewcommand\thesubsection{(\alph{subsection})}

  - \renewcommand{\headrulewidth}{0pt}
  - \renewcommand{\arraystretch}{1.7} % Adjust the value as needed


  - \usepackage{epigraph}
  - \setlength\epigraphwidth{.6\textwidth}

  - \setmainfont{Georgia}
  - \usepackage{titlesec}
  - \newfontfamily\headingfont{Avenir}
  - \titleformat{\chapter}[display]{\headingfont\large}{\chaptertitlename\ \thechapter}{20pt}{\Huge}
  - \titleformat*{\section}{\Large\headingfont}
  - \titleformat*{\subsection}{\large\headingfont}
  - \titleformat*{\subsubsection}{\large\headingfont}


  - \hyphenation{Web-Assembly}


---

```{r setup, include = FALSE}
library(ggplot2)
library(dplyr)
library(scales)
```

<!-- -------------------- -->
<!--      COVER PAGE      -->
<!-- -------------------- -->



\begin{titlepage}
\thispagestyle{fancy}
\fancyfoot[C]{2024}
\centering
{\Huge\bfseries Nebula\par}

\vspace{0.5cm}

{\Huge\itshape Comparing two waves of cloud compute\par}

\vspace{1cm}

{\Large Marius Nilsen Kluften\par}

\vspace{1cm}

\includegraphics[width=0.4\textwidth]{assets/uio-logo.png}\\[1cm]

{\Large Thesis submitted for the degree of\par}
{\Large Master in Informatics: Programming and System Architecture\par}

\vspace{0.25cm}

{\Large 60 credits\par}

\vspace{1cm}

{\Large Institute of Informatics\par}

{\Large Faculty of mathematics and natural sciences\par}

{\Large University of Oslo\par}

\end{titlepage}


<!-- -------------------- -->
<!--      TITLE PAGE      -->
<!-- -------------------- -->

\newpage

\begin{center}
\vspace*{\fill} 
\Huge \textbf{Nebula} \\[10pt]
\vspace{2cm} 
\Large \textit{Comparing two waves of cloud compute} \\[20pt]
\vspace{2cm} 
\Large Marius Nilsen Kluften \\[30pt]
\vspace*{\fill} 
\end{center}


<!-- -------------------- -->
<!--    COPYRIGHT PAGE    -->
<!-- -------------------- -->

\newpage
\listoftodos
\newpage


\null\vfill
\begin{flushleft}
\Large © 2024 Marius Nilsen Kluften\par
\Large Nebula \par
\Large https://duo.uio.no/ \par
\Large Printed: Reprosentralen, University of Oslo \par
\end{flushleft}

\newpage
\fancyfoot[C]{ \thepage\ }
\pagenumbering{roman}

\vspace*{\fill}
\begin{center}
\begin{minipage}{0.75\textwidth}

\addcontentsline{toc}{chapter}{Abstract}
\chapter*{Abstract}

The ever increasing demand for cloud services has resulted in the expansion of
energy-intensive data centers, the ICT industry accounts for about 1 \% of global 
electricity use, highlighting a need for sustainable options
in cloud computing architectures.

\vspace{0.25cm}

This thesis investigates WebAssembly, a technology originally intended for running 
in the browser, as a potential contender in the space of technologies to
consider in cloud native applications. Leveraging the inherent efficiency,
portability and lower startup times of WebAssembly modules, this thesis presents an
approach that aligns with green energy principles, while maintaining performance and
scalability, essential for cloud services. 

\vspace{0.25cm}

Preliminary findings suggest that programs compiled to WebAssembly modules have 
reduced startup and runtimes, which hopefully leads to less energy
consumption and offering a viable pathway towards a more sustainable cloud.

\end{minipage}
\end{center}
\vspace*{\fill}

\newpage

\vspace*{\fill}
\begin{center}
\begin{minipage}{0.75\textwidth}

\addcontentsline{toc}{chapter}{Acknowledgements}
\chapter*{Acknowledgments}


The idea for the topic for this thesis appeared in an episode of the podcast
"Rustacean station". Matt Butcher, the CEO of Fermyon, told the story of his
journey through the different waves of cloud computing, and why Fermyon 
decided to bet on WebAssembly for the next big wave of cloud compute.

\vspace{0.25cm}

The capabilities of WebAssembly running on the server, with the aid of
the WebAssembly System Interface project, caught my interest and started 
the snowball that ended up as the avalanche that is this thesis. 

\vspace{0.25cm}

I'd like to thank Matt Butcher and the people over at Fermyon for inadvertently 
inspiring my topic. 

\vspace{0.25cm}

Furthermore I'd like to thank my two supervisors Joachim Tilsted Kristensen and 
Michael Kirkedal Thomsen, whom I somehow managed to convinced to help guide me 
through such a cutting edge topic. Their guidance and insight have been
invaluable the past semesters. 

\vspace{0.25cm}


Finally I would like to thank Syrus Akbary, founder of Wasmer, whom I met at
WasmIO 2024 who showed me how to reduce my startup times by a further 100 times.

\end{minipage}
\end{center}
\vspace*{\fill}

\newpage



\tableofcontents

\newpage

\listoffigures

\newpage

\listoftables

\newpage

\pagenumbering{arabic}

\part{Overview}


\newpage

\chapter{Introduction}
\label{chap:intro}

\epigraph{\itshape 
  If WASM+WASI existed in 2008, we wouldn't have needed to created Docker. 
  That's how important it is. Webassembly on the server is the future of computing.
  A standardized system interface was the missing link. 
  Let's hope WASI is up to the task!
}{---Solomon Hykes, \textit{Founder of Docker}}

In the digital age, cloud computing has emerged as a foundational technology in
the technological landscape, driving innovation and increased efficiency across
various sectors. Its growth over the past decade has not only transformed how
consumers store, process, and access data, but it has also raised environmental
concerns as more and more data centers are built around the globe to accommodate
the traffic, consuming vast amounts of power. The Information and Communication
Technology (ICT) industry, with cloud computing at its core, accounts for an
estimated 2.1\% to 3.9\% of global greenhouse gas emissions. Data centers, the
backbone of cloud computing infrastructures, are responsible for about 200
TWh/yr, or about 1\% of the global electricity consumption, a figure projected
to escalate, potentially reaching 15\% to 30\% of electricity consumption in
some countries by 2030 \citep{freitag2021}.

The sustainability of cloud computing is thus under scrutiny, and while some
vendors strive to achieve a net-zero carbon footprint for their cloud computing
services, many data centers still rely on electricity generated by fossil fuels,
a leading contributor to climate change \citep{mytton2020}. This reality
emphasizes an urgent need to explore alternative technologies that promise
enhanced energy efficiency while meeting customers demands. In this vein,
serverless computing has emerged as a compelling paradigm, offering scalability
and flexibility by enabling functions to execute in response to requests, rather
than having a server running all the time. However, the inherent startup latency
associated with containerized serverless functions pose a challenge,
particularly for on-demand applications. To mitigate this, vendors
often opt for keeping the underlying servers *warm* to keep the startup latency
as low as possible for serving functions. Reducing the startup time for serving
a function should reduce the need for keeping servers warm and therefore reduce
the standby power consumption of serverless architectures.

\section{Motivation}
\label{sect:motivation}

The environmental footprint of cloud computing, particularly the energy demands
of data centers, is a pressing issue. As the digital landscape continues to
evolve, the quest for sustainable solutions has never been more critical. This
thesis is motivated by the need to reconcile the growing demand for cloud
services with the pressing need for environmental sustainability. Through the
lens of WebAssembly and \ac{WASI}, this thesis aims to investigate innovative
deployment methods that promise to reduce energy consumption without sacrificing
performance, thereby contributing to the development of a more sustainable cloud
computing ecosystem.

\section{The Project}
\label{sect:project}

This thesis explores \ac{Wasm} with \ac{WASI} as an
innovative choice for deploying functions to the cloud, through developing a
prototype \ac{FaaS} platform named Nebula. This platform will run
functions compiled to \ac{Wasm}, originally designed for high-performance
tasks in web browsers, which coupled with \ac{WASI}, allows us to give WebAssembly
programs access to the underlying system. This holds potential for a more
efficient way to package and deploy functions, potentially reducing the startup
latency and the overhead associated with traditional serverless platforms.
\ac{Wasm} and \ac{WASI} offers a pathway, where the demands of today is met, while
reducing the carbon footprint for cloud applications.

\section{Problem Statement}
\label{sect:problems}

The goal of the thesis is to:

1. Develop a prototype cloud computing platform for the \ac{FaaS} paradigm.
2. Use this platform for conducting experiments that either prove or disprove
   the claim that WebAssembly is the more energy efficient choice.

By achieving these goals this thesis seeks to shed light on the feasibility and
implications of adopting \ac{Wasm} and \ac{WASI} for a greener cloud.

\section{Outline}

The thesis has five chapters; this introduction, a chapter that goes
through the background for how cloud computing got to this point, a chapter
dedicated to the process of building Nebula, a chapter for discussing the
results from the experiments, and ending with a chapter suggesting future works.

\todo[inline=true]{Feedback from Michael: When I write this, use it to point the reader to
the most important parts of the thesis. Avoid listing the chapters; you already
have a table-of-contents.}

\newpage


\chapter{Three waves of cloud compute}
\label{chap:three-waves}

\setlength\epigraphwidth{.35\textwidth}

\epigraph{\itshape 
9 out of 10 cloud providers hate this one simple trick.
}{Joachim, my supervisor}

The evolution of cloud computing represents a transformative adventure, driven by
the pursuit for efficiency, scalability and reliability, yet it also
poses challenges, notably it's environmental impact. This chapter steps through this
adventure by introducing the concept of the "Three waves of cloud computing", coined
by the WebAssembly community \citep*{butcherDodds2024}. Where the two first waves of
cloud compute represent the shit from Virtual Machines to Containerization, the
third wave encompasses utilizing \ac{Wasm} and \ac{WASI} to build the next era of cloud compute with the potential to
significantly reduce the carbon footprint.

\section{Ashore: Before the waves}
\label{sect:ashore}

Before delving into the waves themselves, it is essential to understand the
landscape that preceded cloud computing. Prior to the cloud era, companies were
required to building and mantaining their digital services in-house. This
required companies to invest heavily into both expensive hardware and expensive
engineers to buy, upkeep and oversee their own physical servers and network
hardware. (See \Cref{fig:myspeis} for an example)


\begin{figure}[H]
\centering
  \includegraphics[width=0.7\columnwidth]{assets/2.1-before-the-waves-2.png}
  \caption{Example of a company that host their own infrastructure.}
  \label{fig:myspeis}
\end{figure}

This setup mandates a significant upfront costs involved in setting up and
maintaining such an infrastructure, which puts a considerable financial strain
on organizations, and kept smaller companies that were unable to invest in this,
at an disadvantage. 

As a response to this, some companies found a market for taking on the
responsibility of managing infrastructure, and offer \ac{IaaS} to an evolving
market that relies more and more on digital solutions. \ac{IaaS} provides
consumers with the ability to provision computing resources where they can
deploy and run software, including operating systems and applications
\citep{nist-def}. On these managed infrastructures companies could deploy their
services on top of virtual machines that allowed more flexibility, and lowered
the bar to new companies.

\section{The First Wave: Virtual Machines}
\label{sect:first-wave}

The start of cloud computing can be traced back to the emergence of
virtualization, more specifically virtual machines, a response to the costly and
complex nature of managing traditional, on-premise data centers. During the
mid-2000s, Amazon launched its subsidiary, Amazon Web Services (AWS), who in
turn launched Amazon S3 in March 2006, followed by Elastic Compute Cloud (EC2)
in August the same year \citep{barrAmazonEC2Beta2006}. With these services, AWS
positioned itself as a pioneer in this space, marking a major turning point in
application development and deployment, and popularized cloud computing. EC2, as
an \ac{IaaS} platform, empowered developers to run virtual machines remotely.
(See \Cref{fig:feisbook} for an example of this kind of architecture)

\begin{figure}[H]
\centering
  \includegraphics[width=0.7\columnwidth]{assets/2.2-first-wave-2.png}
  \caption{Example of ``Fæisbook`` building their services on EC2 and using S3
for storage.}
  \label{fig:feisbook}
\end{figure}

While similar services existed before 2006, with Amazon's existing large
customer base helped them gain significant traction, and ushered in a the first
era, or wave, of *cloud computing*.

\section{The Second Wave: Containerization}
\label{sect:second-wave}

As we entered the 2010s, the focus shifted from virtual machines to containers,
largely due to the limitations of VMs in efficiency, resource utilization, and
application deployment speed. Containers, being a lightweight alternative to
VMs, designed to overcome these hurdles \citep{bao2016}. 

In contrast to VMs, which require installation of resource-intensive operating
systems and minutes to start up, containers along with their required OS
components, could start up in seconds. Typically managed by orchestration tools
like Kubernetes^[https://kubernetes.io], containers enabled applications to
package alongside their required OS components, facilitating scalability in
response to varying service loads. Consequently, an increasing number of
companies have since established platform teams to build orchestrated developers
platforms, thereby simplifying application development in Kubernetes clusters.
(See \Cref{fig:docker} for an example where a fictive company ``WacDonalds`` and
their container workflow)

\begin{figure}[H]
\centering
  \includegraphics[width=0.9\columnwidth]{assets/2.3-second-wave-2.png}
  \caption{DevOps engineer deploying services as containers on \ac{GCP}.}
  \label{fig:docker}
\end{figure}

Containers are not a perfect solution however, and while they simplify the means
of developing and deploying applications, docker images can easily reach
Gigabytes in image size \citep{durieux2023}, can take a long time to start up,
and building applications that target multiple platforms can be difficult.

These solutions are more efficient than manually installing an operating system
on a machine, but they still have leave a large footprint. Is there a more
efficient way to package and deploy our programs? \ac{Wasm} and \ac{WASI}, as
mentioned in epigraph of \cref{chap:intro}, has positioned itself as a potential
contender for how applications are built, packaged and deployed to the cloud.

\section{The Third Wave: WebAssembly}
\label{sect:third-wave}

WebAssembly has had a surge of popularity the past three to four years when
developers discovered that what it was designed for - to truly run safely inside
the browser - translated well into a cloud native environment as well.
Containers have, with the benefits mentioned in \cref{sect:second-wave}, had a
positive impact on the cloud native landscape. However, with the limitations -
like large image sizes, slow startups and complexity of cross-platform - there
is space for exploring alternative technologies for building our cloud-native
applications.

WebAssembly is a compilation target with many languages adopting support, and by
itself, it is sandboxed to run in a WebAssembly \ac{VM} without access to the outside
world, meaning that it cannot access the underlying system. This means that a
"vanilla" WebAssembly module cannot write to the file system, update a Redis
cache or transmit a POST request to another service. 

To make this possible, the WebAssembly System Interface project was created.
This project allows developers to write code that compiles to WebAssembly that
can access the underlying system. This is the key project that turned many
developers onto the path of exploring WebAssembly as a potential contender for
building cloud applications. With WebAssembly, developers can write programs in
a programming language that supports it as a compilation target, and build tiny
modules that can run on a WebAssembly runtime. These WebAssembly runtimes can
run on pretty much any architecture with ease, the resulting binary size are
quite small, and the performance is near-native. These perks combined with the
potential for reduced overhead, smaller image sizes, and faster startup times
make WebAssembly and \ac{WASI} a promising candidate for the third wave of cloud
compute with a lower impact on the environment.

In summary, the three waves of cloud computing - virtual machines, containers,
and now \ac{Wasm} with \ac{WASI} - represent the industry's pursuit of more
efficient, scalable and reliable solutions for building cloud applications.
While each wave has attempted to tackle pressing challenges of its time, it is
exciting to see how \ac{Wasm} and \ac{WASI} can be leveraged in this third wave and
see if it is promise of more efficient applications can lead to reducing the
environmental impact of ICT.


\newpage

\chapter{Background}
\label{chap:background}

\epigraph{\itshape 
Data has gravity, and that gravity pulls hard
}{David Flanagan}

\section{Cloud Computing Overview}
\label{sect:cloud-overview}

Cloud computing, commonly referred to as "*the cloud*", refers to the delivery of
computing resources served over the internet, as opposed to traditional
on-premise hardware setups. The \ac{NIST} defines cloud computing like so: 


\begin{tcolorbox}[
  definitionstyle,
  title=NIST definition of Cloud Computing,
]
Cloud computing is a model for enabling ubiquitous, convenient, on-demand
network access to a shared pool of configurable computing resources (e.g.,
networks, servers, storage, applications, and services) that can be rapidly
provisioned and released with minimal management effort or service provider
interaction. \\

\hfill \citep{nist-def}

\end{tcolorbox}

Cloud computing traces its root back to the 1960s, with the Compatible
Time-Sharing System (CTSS) project at MIT, which demonstrated the potential for
multiple users accessing and sharing computing resources simultaneously
\citep{crisman1963}. While CTSS was a localized system, it paved the way for the
concept of shared computing resources, a fundamental principle of cloud
computing.

Over the following decades, advancements in networking, virtualization and the
ubiquity of the internet led to the development of today's sophisticated cloud
services. The term "cloud computing" was first coined in the year 1996 by
Compaq, \citep{favaloroInternetSolutionsDivision1996}, but it was not until
Amazon launched its subsidiary \ac{AWS} in the 2006 that the adoption
became wide spread. 

The launch of AWS's Amazon S3 cloud service in March 2006, followed by Elastic
Compute Cloud (EC2) in August the same year \citep{barrAmazonEC2Beta2006},
marked a major turning point in application development and deployment, and
popularized cloud computing. EC2, as an Infrastructure-as-a-Service platform,
empowered developers to run virtual machines remotely. By providing these
services over the internet on a pay-as-you-go basis, AWS drastically lowered the
bar for accessing computing resources, making it easier and more cost-effective
for businesses and developers to build and deploy applications without the need
for considerable upfront investment in hardware and infrastructure.

With the success of AWS, other major technology companies saw their fit to enter
the cloud computing market. In 2008, Google launched the Google App Engine
\citep{mcdonaldIntroducingGoogleApp2008}, a platform for building and hosting
web applications in Google's data centers. Microsoft followed with the launch of
Azure in 2010, its cloud computing platform that offers a range of services
comparable to AWS.

The rapid growth of cloud computing also fueled the rise of DevOps practices and
containerization technologies like Docker, which facilitate the development,
deployment and management of applications on the cloud. Orchestration tools
like Docker Swarm and Kubernetes further simplify the process of managing and
scaling containerized applications across cloud enviroments
\citep{bernsteinContainersCloudLXC2014}. 

Today, cloud computing has become an essential part of modern IT infrastructure,
where major cloud provides, like AWS, Microsoft and Google, continue to innovate
and expand their offerings. Cloud computing has also enable new paradigms like
serverless computing and edge computing, allowing for even more efficient and
distributed computing models. \citep{baldiniServerlessComputingCurrent2017}

\section{Energy consumption and Sustainability in Cloud Computing}

One of the downsides to contrast the benefits of cloud computing, is the ever
increasing demand for energy consumption required for running these servers in
giant data centers. With the increased demand for energy consumption, comes an
increased impact on the environment. As mentioned in \cref{chap:intro}, the
ICT industry accounts for an estimated 2.1% to 3.9% of global greenhouse
emissions  \citep{freitag2021}. According to the International Energy Agency
(IEA), data centers across the globe consumed between from 240 to 340 TWh,
accounting for 1 to 1.3% of the global electricity use. 

In Norway, for example, Google is constructing a data center in Skien, expected
to be fully operational by 2026. As of April 2024, they have been granted a
capacity of 240 Megawatts, but they have applied for a total capacity of 860
Megawatts \citep{rivrudInvesteringenAvGooglesenter2024}. At full capacity at 860
MW, Google's data center is aiming to consume 7.5 TWh each year, and according
to Google's most recent sustainability report, they consumed a total of 22.29
TWh globally in 2022 \citep{Google2023Environmental2023}. In other words, in
2026 the data center in Skien alone is projected to consume ~33% of the energy
Google consumed globally in 2022. The energy consumption in Norway is projected
to reach 150-158 TWh in 2026 \citep{gunnerodStatnettAnalyse2022}, meaning that
the data center in Skien could account for 5\% of the energy consumption in the
country. \Cref{fig:skienvsnorway} below illustrates the amount of energy the new
data center will consume compared to Norway.

\vspace{0.25cm}

```{r, skienvsnorway,echo=F, eval=T, out.width="80%", fig.align = 'center', fig.cap="Projected energy consumption in 2026: Skien data center and Norway."}
# Create a data frame with energy consumption values
energy_data <- data.frame(
  Category = c("Google Data center in Skien", "Rest of Norway"),
  Energy_Consumption = c(7.5, 150) # Assuming Norway's total consumption is 140 TWh/year
)

# Create a pie chart with labels
ggplot(energy_data, aes(x = "", y = Energy_Consumption, fill = Category)) +
  geom_col() +
  geom_label(aes(label = c("7.5 TWh", "150 TWh"), x = 1.3),
    position = position_stack(vjust = 0.5),
    color = "#eeeeee",
    fontface = "bold"
  ) +
  coord_polar("y", start = 0) +
  theme_void() +
  labs(
    fill = "Estimated energy consumption 2026"
  ) +
  scale_fill_manual(values = c("#1a338f", "#FF7F0E"))
```

\todo[inline=true]{Consider this feedback from Michael: Google also does not
make it rain more. If Google come and buys all the green energy in the area, it
will just mean that others will use more black energy. They don't build new
infrastructure. And this is only for Googles "usage". Not the users and network
infrastructure.}

The flipside of this, is that Google is commited to reach a net-zero carbon
footprint by 2030, and the data center in Skien is built to reflect this. Google
has been carbon neutral since 2007 and has matched 100% of its annual
electricity consumption with renewable energy since 2017
\citep{googleTrackingOurCarbonFree}. Norway's abundant hydropower, rising wind
power production, investment into solar energy and other renewable energy
sources make it an ideal location for building data centers aiming to be powered
by renewable energy \citep{norwegian-energyElectricityProduction2023}. 

Like Google, other major cloud providers have set ambitious targets for
renewable energy adoption and have invested in large-scale renewable projects.
Microsoft has committed to shifting to 100% renewable energy by 2025 for all its
data centers, buildings and campuses, and to be carbon *negative* by 2030
\citep{smithMicrosoftWillBe2020}, while Amazon has pledged to transition to 100%
renewable energy by 2030 for its cloud subsidiary and to have a net-zero carbon
footprint by 2040 \citep{amazonClimatePledge2019}. These efforts have
contributed to reducing greenhouse gas emissions in the cloud computing
industry, but this commitment is not universally adopted, and many data centers
still rely on electricity generated by fossil fuels, a leading contributor to
climate change \citep{mytton2020}.

Several factors make up the energy consumption required to service a data
center. One of these factors is cooling down the servers while running, and a
study from 2017 discovered that cooling accounted for about 38% of total energy
consumption in data centers, ranging from 21% to 61% depending the effectiveness
of the facility's heating, ventilation, and air conditioning (HVAC) system
\citep{niReviewAirConditioning2017}.

One innovative example of attempting to mitigate the environmental impact of
cooling data centers can be found by data center providers like DeepGreen
^[https://deepgreen.energy/faqs/], who submerge their servers into dielectric
fluid, which gets warmed up by the excess heat of the computers. This heat is
then transferred to a host's hot water system via a heat exchange and used for
heating up swimming pools in London. Another broad strategy cloud providers
opt for is the implementation of power management techniques, such as dynamic
voltage and frequency scaling (DVFS), which adjusts the power consumption of
servers based on workload demands \citep{beloglazovEnergyawareResourceAllocation2012}.

Virtualization and resource pooling, two key components of cloud computing, also
contribute to energy efficiency. By consolidating virtual machines onto shared
physical servers, cloud providers are able to improve resource utilization and
reduce the energy consumption of their data centers.
\citep{beloglazovEnergyawareResourceAllocation2012} 

\section{Virtualization and Virtual Machines}
\label{sect:virtual}

Virtualization is the process of creating a virtual version of a physical
resource, such as an operating system, a server, a storage device, or a network
resource \citep{chiuehSurveyVirtualizationTechnologies2005}. Virtualization
allows multiple virtual instances to share the underlying physical hardware,
enabling more efficient resource utilization and consolidation.

One of the most common forms of virtualization is the creation of \ac{VMs}.
\citet{barhamXenArtVirtualization2003} described that a virtual machine is a
software-based emulation of a physical computer system, including its processor,
memory, storage, and network interfaces. Furthermore they describe that \ac{VMs} run
on top of a hypervisor, a software layer that manages and allocates the physical
hardware resources to the virtual machines.

\Cref{vm-figure} below illustrates virtual machines running in such an
environment.

\begin{figure}[H]
\centering
  \includegraphics[width=0.7\columnwidth]{assets/3.2-vm-figure.png}
  \caption{Virtual machines running on top of Hypervisor on a computer.}
  \label{vm-figure}
\end{figure}


\section{Containers and Container orchestration}
\label{sect:containers}

While virtual machines provide isolation at the hardware level, containers offer
a more lightweight form of virtualization by isolating applications at the
operating system level \citep{merkelDockerLightweightLinux2014}. Containers
share the host operating system's kernel, enabling them to be more lightweight
and efficient compared to traditional virtual machines. They can run physical
servers, as well as on \ac{VMs} \citep{bernsteinContainersCloudLXC2014}.

\citet{merkelDockerLightweightLinux2014} also explains that containers package
an application, along with its dependencies, libraries, and configuration files,
into a single, self-contained unit. This allows applications to be deployed
consistently across different environments, ensuring predictable behavior and
reducing compatibility issues \citep{sergeevDockerContainerPerformance2022}

Docker is one of the most widely adopted container platforms, providing tools
for building, shipping, and running applications in containers
\citep{merkelDockerLightweightLinux2014}. Docker containers are based on open
standards and can run on various operating systems and cloud platforms
\citep{sergeevDockerContainerPerformance2022}. \Cref{container-figure} below
illustrates how containers run ontop of a Docker Engine on a virtual or physical
machine.

\begin{figure}[H]
\centering
  \includegraphics[width=0.7\columnwidth]{assets/3.3-container-figure.png}
  \caption{Containers running on the Docker Engine}
  \label{container-figure}
\end{figure}

As the number of containers in an environment grows, managing and orchestrating
them becomes increasingly complex. Container orchestration tools, such as
Kubernetes and Docker Swarm, help automate the deployment, scaling, and
management of containerized applications across multiple hosts
\citep{burnsBorgOmegaKubernetes2016}.

These tools provide features like:

1. Automated deployment and scaling: Containers can be automatically
   provisioned, scaled up or down based on demand, and load-balanced across
multiple hosts \citep{burnsBorgOmegaKubernetes2016}.
2. Self-healing and monitoring: Orchestration tools can monitor the health of
   containers and automatically restart or reschedule them in case of failures
\citep{kubernetesKubernetes}.
3. Service discovery and load balancing: Applications running in containers can
   be easily discovered and accessed by other services, enabling microservice
architectures \citep{kubernetesKubernetes}.

Container orchestration has become an essential component of modern cloud-native
architectures, enabling efficient management and scaling of containerized
applications in dynamic environments.


\section{Serverless Computing}

Serverless computing has emerged as an alternative to traditional infrastructure
management approahces, such as managing physical servers or building developer
platforms as described in \Cref{sect:containers}
\citep{baldiniServerlessComputingCurrent2017}. In serverless computing, the
underlying infrastrucutre is abstracted away, allowing developers to focus on
writing code and deploying programs without the need to provision or manage
servers
\citep{baldiniServerlessComputingCurrent2017,robertsServerlessArchitectures2018}.
\citet{castroRiseServerlessComputing2019} defines serverless as such: 

\begin{tcolorbox}[
  definitionstyle,
  title=Serverless definition,
]
Serverless computing is a platform that hides server usage from
developers and runs code on-demand automatically scale and billed only for the
code running. \\

\hfill \citep{castroRiseServerlessComputing2019}

\end{tcolorbox}


According to \citet{baldiniServerlessComputingCurrent2017}, in a serverless
model, the cloud provider is responsible for managing the infrastructure,
including resource allocation, scaling, and maintenance. This approach enables
more efficient resource utilization, as resources are dynamically allocated
based on demand. Furthermore, developers can deploy containers or functions
independently, promoting modularity and scalability.

On top of this, severless computing offers several more benefits, including:

1. Reduced operational overhead: Developers do not need to manage servers or
   infrastructure, freeing up time and resources for application development
\citep{baldiniServerlessComputingCurrent2017}.
2. Faster time-to-market: With serverless, developers can quickly deploy and
   iterate on functions without the need for time consuming infrastructure setup
\citep{adzicServerlessComputingEconomic2017}.
3. Cost efficiency: Serverless platforms typically employ a pay-per-use pricing
   model, where users are charged based on the actual execution time and
resources consumed by their applications \citep{eismannReviewServerlessUse2021}. 

However, serverless architectures also introduce certain challenges, such as: 

1. Potential vendor lock-in: Serverless platforms may have provider-specific
   APIs and services, which can make it challenging to change providers or
migrate applications \citep{gottliebServerlessDataLockin2018}.
2. Cold start latencies: Containers or functions that have not beeen invoked
   recently may experience longer startup times, cold starts, which can impact
application performance \citep{golecColdStartLatency2023}.
3. Resource overhead and efficiency: Serverless platforms typically rely on
   containerization technologies, such as Docker, to encapsulate and isolate
function executions. The use of containers can introduce resource overhead and
impact the efficiency of serverless applications
\citep{akkusSANDHighPerformanceServerless2018}.

Examples of widely used platforms that build on the serverless model can be
found at the major cloud providers, like Google Cloud
Run^[https://cloud.google.com/run], \ac{AWS}
Fargate^[https://aws.amazon.com/fargate/] and Microsoft's
\ac{ACI}^[https://azure.microsoft.com/en-us/products/container-instances]. These
three example platforms allow developers to deploy containers onto the cloud
without worrying about orchestration behind the scenes.

\subsection{Functions-as-a-Service}

\ac{FaaS} is a cloud computing model, derived from serverless, that allows
developers to execute individual functions in response to events or triggers
without needing to manage the underlying infrastructure
\citep{sewakWinningEraServerless2018}. 

Examples of these \ac{FaaS} platforms among the big three cloud providers are;
\ac{AWS} Lambda^[https://aws.amazon.com/lambda/], Google Cloud
Functions^[https://cloud.google.com/functions], and Microsoft's Azure
Functions^[https://azure.microsoft.com/en-us/products/functions]. On FaaS
platforms like these, developers write and deploy small, self-contained
functions that perform specific tasks. These functions are typically stateless
and can be written in various programming languages supported by the FaaS
provider \citep{baldiniServerlessComputingCurrent2017}.

When a function is triggered, the FaaS platform automatically allocates the
necessary resources to execute the function, such as CPU, memory, and network
bandwidth. The platform also handles the scaling of the function based on the
incoming requests, ensuring that the function can handle varying workloads by
itself \citep{mcgrathServerlessComputingDesign2017}.

\todo{add citations to rest of text}

FaaS platforms often utilize container technology, like Docker, to provide an
isolated environment for each function execution. Containers offer several
benefits, such as fast startup times, efficient resource utilization, and the
ability to package functions with their dependencies
\citep{vaneykServerlessMorePaaS2018}. However, the use of containers in FaaS
also introduce some challenges including, cold start latency and the overhead
associated with container initialization and management
\citep{wangPeekingCurtainsServerless2018}.


Cold start latency refers to the time it takes for a FaaS platform to provision
a new container instance when a function is invoked after a period of
inactivity. This latency can be significant, especially for applications with
strict performance requirements \citep{wangPeekingCurtainsServerless2018}. The
limitations of containers in FaaS environments have led to the exploration of
alternative approaches, such as using \ac{Wasm} and \{WASI}. As Solomon Hykes,
the creator of Docker, stated: 

\begin{displayquote}
\textit{ ``If WASM+WASI existed in 2008, we wouldn't have needed to create Docker.
That's how important it is. WebAssembly on the server is the future of cloud
computing. A standardized system interface was the missing link. Let's hope WASI
is up to the task.`` \citep{hykesOne2019} }
\end{displayquote} 

This statement highlights the potential of \ac{Wasm} and \ac{WASI} to face the
challenges with containers in FaaS and to provide a more efficient and
platform-agnostic approach to serverless computing, a potential explored and
supported by \citet{kjorveziroskiEvaluatingWebAssemblyOrchestrated2022}.

\section{WebAssembly and WASI}
\label{sect:wasmwasi}

WebAssembly, commonly referred to as Wasm, is a modern binary instruction format
that has risen to prominence as a versatile technology across a diverse amount
of computing environments, originating in the web browser. This section
introduces the project that WebAssembly evolved from - *asm.js* - and illustrate
how WebAssembly lets developers write programs in a high-level language and
run them across a multitude of platforms.


\subsection{asm.js}
\label{subsect:asm}

Mozilla released the first version of asm.js in 2013 and designed it to be a
subset of JavaScript, designed to allow web applications written in other
languages than JavaScript, such as C or C++, to run in the browser. The
intention of asm.js is to allow for web applications to run at performance
closer to native code than applications written in standard JavaScript can
achieve. A simplified flow for how source code written in C/C++ is compiled to
bytecode that can be executed in the browser can be found in figure
\ref{fig:asm-figure} below.

\begin{figure}[H]
\centering
  \includegraphics{assets/asm.js-figure.png}
\caption{Source code in C/C++ compiled to asm.js and run in browser}\label{fig:asm-figure}
\end{figure}

While asm.js was a great leap forward, being a subset of JavaScript limited the
scope of what it could become, leading to its deprecation in 2017 and the
development of a more efficient and portable format \citep{webassembly.orgFAQWebAssembly}. 


\subsection{WebAssembly}

The team at Mozilla built upon the lessons learned from asm.js and went on to
develop WebAssembly and launch the first public version in
2017. 
WebAssembly, which is a low-level code format
designed to serve as a compilation target for high-level programming languages.
It is a binary format that gets executed by a stack-based virtual machine,
comparable to how Java bytecode runs on the \ac{JVM}. 

\begin{figure}[H]
\centering
  \includegraphics{assets/wasm-browser.png}
  \caption{Source code compiled to WebAssembly and embedded in browser}
  \label{fig:wasm-browser}
\end{figure}

\newpage

\chapter{Ignore the below for now, it is not ready for review}

WebAssembly, originally designed for running demanding computations in web
browsers, present a promising technology that could help reduce the energy
consumption of cloud services. It offers an interesting option for packaging
functions with its compact binary format and fast execution time. This has the
potiential to significantly reduce startup latency and resource overhead
associated with traditional serverless platforms. This increased efficiency
could lead to a direct decrease in energy consumption for cloud services, which
in turn could motivate the industry to adopt alternative technology that enable
a more sustainable cloud.

\todo{Rewrite the rest of this section, it's a bit of a mess right now}

The WebAssembly team defines WebAssembly as such:

\begin{tcolorbox}[
  definitionstyle,
  title=WebAssembly definition,
]
WebAssembly (abbreviated Wasm) is a binary instruction format for a stack-based
virtual machine. Wasm is designed as a portable compilation target for
programming languages, enabling deployment on the web for client and server
applications.

\hfill \href{https://webassembly.org}{webassembly.org}

\end{tcolorbox}

In other words, WebAssembly is a low-level code format designed to serve as a
compilation target for high-level programming languages. It's a binary format
that gets executed by a stack-based virtual machine, similar to how Java
bytecode runs on the Java Virtual Machine (JVM). It was originally designed for
running in a browser environment, and every major browser has implemented a way
for running it. 


WebAssembly have promising properties that makes it interesting to investigate
if it can find a home outside the browser environment it was designed for:

\textit{Efficiency and Speed}: WebAssembly was designed to be fast, enabling
near-native performance. Its binary format is compact and designed for quick
decoding, contributing to quicker startup times, important aspects of cloud
native applications.

\textit{Safety and Security}: WebAssembly is designed to run safely in a secure
sandbox. Each WebAssembly module executes within a confined environment without
direct access to the host system's resources. This isolation of processes is
inherent in WebAssembly's design, promoting secure practices. 

\textit{Portability}: WebAssembly's platform-agnostic design makes it highly
portable. It can run across a variety of different system architectures. For
cloud native applications, this means WebAssembly modules, once compiled, can
run anywhere - from the edge to the server - on any environment. 

\textit{Language Support}: A large amount of programming languages can already
target WebAssembly. This means developers are not restricted to a particular
language when developing applications intended to be deployed as WebAssembly
modules. This provides greater flexibility to leverage the most suitable
languages for particular tasks. 



In contrast, traditional methods such as deployment with containers or VMs can
be resource-intensive, slower to boot up, less secure due to a larger surface
attack area, and less efficient. Given these, WebAssembly, with its efficiency,
security, and portability, can potentially offer an attractive alternative
deployment method for building and running cloud native applications, like the
"Academemes" service we will explore in this essay.

\subsection{WebAssembly System Interface}

WebAssembly (WASM) and [WebAssembly System Interface (WASI)][16] present
promising choices to traditional ways of deploying and hosting Function as
a Service (FaaS) platforms, offering several notable advantages, in
terms of startup times and energy efficiency.

_Reduced Startup Times_: One of the greatest strengths of Wasm is its compact
binary format designed for quick decoding and efficient execution. It offers
near-native performance, which results in significantly reduced startup times
compared to container-based or VM-based solutions. In a FaaS context, where
functions need to spin up rapidly in response to events, this attribute is
particularly advantageous. This not only contributes to the overall performance
but also improves the user experience, as the latency associated with function
initialization is minimized.

_Improved Energy Efficiency_: Wasm's efficiency extends to energy use as well.
Thanks to its optimized execution, Wasm can accomplish the same tasks as
traditional cloud applications but with less computational effort. The CPU
doesn't need to work as hard, which results in less energy consumed. With data
centers being responsible for a significant portion of global energy consumption
and carbon emissions, adopting Wasm could lead to substantial energy savings and
environmental benefits.

_Scalability_: Wasm's small footprint and fast startup times make it an
excellent fit for highly scalable cloud applications. Its efficiency means it
can handle many more requests within the same hardware resources, hence reducing
the need for additional servers and thus reducing the energy footprint further.

_Portability and Flexibility_: WASI extends the portability of Wasm outside the
browser environment, making it possible to run Wasm modules securely on any
WASI-compatible runtime. This means that FaaS platforms can run these modules on
any hardware, operating system, or cloud provider that supports WASI. This
portability ensures flexibility and mitigates the risk of vendor lock-in.

While runtime efficiency is an important aspect and typically a strength of
Wasm, it might not be the primary focus of this thesis. That being said, it is
worth mentioning that the efficient execution of Wasm modules does contribute to
the overall operational efficiency and energy savings of Wasm-based FaaS
platforms.

In summary, introducing WASM+WASI as a component for deploying and hosting FaaS
platforms can offer significant benefits. Focusing on energy efficiency and reduced startup times, this approach could pave the way for more sustainable,
efficient, and responsive cloud services. In the context of our "Academemes"
service, this could lead to a scalable, performant, and environmentally friendly
platform.

\section{Energy monitoring}

\todo{Write about MQTT and Zwave/Oh my Gude}



\newpage

\part{Project}
\chapter{Approach}

To investigate the problem statements posed in \autoref{sect:problems}, roughly
summarized to exploring if WebAssembly and WebAssembly System Interface can lead
to a more efficient and energysaving way to build our cloud services, an
exploratory approach will be used. Different benchmarking experiments will be
run against a prototype developed for this thesis, where different functions can
be invoked with different inputs and reveal startup and runtimes of invoking
functions compiled to WebAssembly modules and compare these with functions
packaged as Docker images.







\chapter{Analysis}

\newpage

\chapter{Design}

\newpage

\chapter{Implementation}

This is the chapter on implementing Nebula.

\section{Tech stack}

Rust/Docker/Etc.

\newpage

\part{Results}

\newpage
\chapter{Evaulation}

\newpage
\chapter{Discussion}

\newpage
\chapter{Conclusion}

\newpage
\chapter{Future work}
\label{CustomLastPage}



\chapter*{References}

<div id="refs"></div>

[1]: https://dx.doi.org/10.1016/j.patter.2021.100340
[2]: https://doi.org/10.1038/s41558-020-0837-6
[3]: https://www.rust-lang.org/
[4]: https://webassembly.org/
[5]: https://ieeexplore.ieee.org/document/798396
[6]: https://dx.doi.org/10.5381/jot.2009.8.3.c4
[7]: https://www.gartner.com/smarterwithgartner/6-ways-cloud-migration-costs-go-off-the-rails
[8]: https://aws.amazon.com/blogs/aws/amazon_ec2_beta/
[9]: https://dx.doi.org/10.1145/2988336.2988337
[10]: https://kubernetes.io/
[11]: https://aws.amazon.com/lambda/
[12]: https://www.primevideotech.com/video-streaming/scaling-up-the-prime-video-audio-video-monitoring-service-and-reducing-costs-by-90
[13]: https://cloud.google.com/functions
[14]: https://www.infoworld.com/article/3265750/serverless-in-the-cloud-aws-vs-google-cloud-vs-microsoft-azure.html
[15]: https://learn.microsoft.com/en-us/azure/azure-functions/functions-overview
[16]: https://wasi.dev/
[17]: https://www.fermyon.com/spin
[18]: https://luiscruz.github.io/2021/07/20/measuring-energy.html
[19]: https://www.intel.com/content/www/us/en/developer/articles/technical/software-security-guidance/advisory-guidance/running-average-power-limit-energy-reporting.html
[20]: http://dx.doi.org/10.1145/3177754
[21]: https://www.youtube.com/watch?v=9_o2Ia-EIzo

\chapter{Appendices}


\begin{acronym}
  \acro{IaaS}[IaaS]{Infrastructure-as-a-Service}
  \acro{FaaS}[FaaS]{Functions-as-a-Service}
  \acro{Wasm}[Wasm]{WebAssembly}
  \acro{WASI}[WASI]{WebAssembly System Interface}
  \acro{VMs}[VMs]{virtual machines}
  \acro{VM}[VM]{virtual machine}
  \acro{JVM}[VM]{Java Virtual Machine}
  \acro{NIST}[NIST]{National Institute of Standards and Technology}
  \acro{AWS}[AWS]{Amazon Web Services}
  \acro{GCP}[GCP]{Google Cloud Platform}
  \acro{ACI}[ACI]{Azure Container Instances}
\end{acronym}
